{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83600e41",
   "metadata": {},
   "source": [
    "# Projet Machine Learning : Prédiction du Succès des Campagnes de Télémarketing Bancaire\n",
    "\n",
    "**Auteur:** Lyes SID ALI  -  Cédric LES BIENS  -  Nour MAALI  \n",
    "**Date:** Janvier 2026  \n",
    "**Dataset:** Bank Marketing Dataset  \n",
    "**Enseignant:** Dario COLAZZO  \n",
    "\n",
    "## Plan\n",
    "1. Description du Problème et Objectifs\n",
    "2. Import et chargement\n",
    "3. Exploration et Analyse des Données\n",
    "4. Preprocessing et Feature Engineering\n",
    "5. Modélisation\n",
    "6. Évaluation et Comparaison des Modèles\n",
    "7. Validation Finale\n",
    "8. Conclusions et Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca5058",
   "metadata": {},
   "source": [
    "## 1. Description du problème et objectifs\n",
    "### 1.1 Contexte\n",
    "Une institution bancaire portugaise a mené des campagnes de marketing direct basées sur des appels téléphoniques pour promouvoir des dépôts à terme. Ces campagnes ont nécessité de multiples contacts avec les mêmes clients pour déterminer s'ils souscrivaient ou non au produit bancaire proposé.\n",
    "\n",
    "### 1.2 Problème à Résoudre\n",
    "Prédire si un client va souscrire à un dépôt à terme bancaire (variable **y**) en fonction de ses caractéristiques personnelles, du contexte de la campagne et des indicateurs socio-économiques.\n",
    "\n",
    "### 1.3 Type de Problème\n",
    "**Classification Binaire**\n",
    "- Il s'agit d'un problème de classification car la variable cible est catégorielle binaire (yes/no), et ce n'est pas une regression car nous ne prédisons pas une valeur numérique continue\n",
    "- **Objectif :** Classifier chaque client en deux catégories : \n",
    "  - Classe positive (1) : le client souscrit au dépôt (**y = yes**)\n",
    "  - Classe négative (0) : le client ne souscrit pas (**y = no**)\n",
    "\n",
    "### 1.4 Variables et Features\n",
    "\n",
    "**Variable Cible (la target) :**\n",
    "- **y** : Le client a-t-il souscrit à un dépôt à terme ? yes/no\n",
    "\n",
    "**Features (20 variables d'entrée) :**\n",
    "\n",
    "**A. Données du client bancaire (7 features)**\n",
    "- `age` : Age\n",
    "- `job` : Type d'emploi \n",
    "- `marital` : Statut matrimonial \n",
    "- `education` : Niveau d'éducation \n",
    "- `default` : Crédit en défaut  \n",
    "- `housing` : Prêt immobilier \n",
    "- `loan` : Prêt personnel \n",
    "\n",
    "**B. Données du dernier contact de la campagne (4 features)**\n",
    "- `contact` : Type de communication\n",
    "- `month` : Mois du dernier contact \n",
    "- `day_of_week` : Jour de la semaine \n",
    "- `duration` : Durée du dernier contact en secondes \n",
    "\n",
    "**C. Autres attributs de la campagne (4 features)**\n",
    "- `campaign` : Nombre de contacts durant cette campagne \n",
    "- `pdays` : Jours depuis le dernier contact d'une campagne précédente\n",
    "- `previous` : Nombre de contacts avant cette campagne \n",
    "- `poutcome` : Résultat de la campagne précédente \n",
    "\n",
    "**D. Données socio-économiques (5 features)**\n",
    "- `emp.var.rate` : Taux de variation de l'emploi \n",
    "- `cons.price.idx` : Indice des prix à la consommation\n",
    "- `cons.conf.idx` : Indice de confiance des consommateurs\n",
    "- `euribor3m` : Taux Euribor à 3 mois \n",
    "- `nr.employed` : Nombre d'employés \n",
    "\n",
    "### 1.5 Algorithmes à Utiliser\n",
    "\n",
    "Nous allons implémenter et comparer 5 algorithmes différents :\n",
    "\n",
    "1. **Régression Logistique:**  Modèle de classification linéaire simple\n",
    "2. **Decision Tree :**  Algorithme basé sur des règles de décision\n",
    "3. **Naive Bayes:**  Classification utilisant les probabilités basée sur le théorème de Bayes\n",
    "4. **Perceptron:**  Algorithme de classification linéaire\n",
    "5. **K-Nearest Neighbors (KNN):**  Classification basée sur la similarité avec les k voisins les plus proches\n",
    "\n",
    "### 1.6 Métriques d'Évaluation\n",
    "\n",
    "Étant donné qu'il s'agit d'un problème de classification binaire potentiellement déséquilibré, nous utiliserons les métriques :\n",
    "\n",
    "- **Accuracy** : Taux de bonnes prédictions global \n",
    "- **Precision** : Proportion de vrais positifs parmi les prédictions positives \n",
    "- **Recall** : Proportion de vrais positifs parmi tous les positifs réels  \n",
    "- **F1-Score** : Moyenne de Precision et Recall \n",
    "- **Matrice de Confusion** : Visualisation des vrais/faux positifs/négatifs\n",
    "- **Courbe ROC** : Pour visualiser le compromis sensibilité/spécificité\n",
    "\n",
    "### 1.7 Stratégie de Validation (Model Selection)\n",
    "\n",
    "- **Train-Test Split** : 80% entraînement, 20% test\n",
    "- **Validation Croisée K-flod** : 5-fold cross-validation pour évaluer la robustesse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605951aa",
   "metadata": {},
   "source": [
    "## 2. Import et Chargement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             roc_auc_score, confusion_matrix, roc_curve, auc)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "df = pd.read_csv('bank-additional/bank-additional-full.csv', sep=';')\n",
    "print(f\"Dataset chargé : {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3c08f",
   "metadata": {},
   "source": [
    "## 3. Exploration des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc05410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apercu des données\n",
    "print(\"Apercu des données\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infos générales\n",
    "print(\"Infos dataset :\")\n",
    "print(df.info())\n",
    "print(\"\\nStats :\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a0a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"Pas de valeurs manquantes\")\n",
    "else:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution variable cible\n",
    "target_counts = df['y'].value_counts()\n",
    "print(\"Distribution :\")\n",
    "print(target_counts)\n",
    "print(f\"\\nProportions :\\n{df['y'].value_counts(normalize=True)*100}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[0].set_title('Distribution Cible')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 500, str(v), ha='center')\n",
    "\n",
    "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
    "            colors=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1].set_title('Proportions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio = target_counts['no'] / target_counts['yes']\n",
    "print(f\"\\nRatio déséquilibre : {ratio:.1f} (no/yes)\")\n",
    "print(\"Classes très déséquilibrées, faudra en tenir compte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971eff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables catégorielles\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols.remove('y')\n",
    "\n",
    "print(f\"Variables catégorielles : {cat_cols}\\n\")\n",
    "\n",
    "# Nombre de catégories\n",
    "for col in cat_cols:\n",
    "    print(f\"{col}: {df[col].nunique()} catégories\")\n",
    "    \n",
    "# Valeurs inconnues\n",
    "print(\"\\nValeurs unkonwn :\")\n",
    "for col in cat_cols:\n",
    "    unknown = (df[col] == 'unknown').sum()\n",
    "    if unknown > 0:\n",
    "        print(f\"{col}: {unknown} ({unknown/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d097e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation variables catégorielles\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "cols_plot = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "for idx, col in enumerate(cols_plot):\n",
    "    counts = df[col].value_counts()\n",
    "    axes[idx].barh(range(len(counts)), counts.values)\n",
    "    axes[idx].set_yticks(range(len(counts)))\n",
    "    axes[idx].set_yticklabels(counts.index)\n",
    "    axes[idx].set_title(col)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6be854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables numeriques\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Variables numériques : {num_cols}\\n\")\n",
    "df[num_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution variables numériques\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(num_cols):\n",
    "    axes[idx].hist(df[col], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(col)\n",
    "    axes[idx].set_ylabel('Fréquence')\n",
    "    \n",
    "for idx in range(len(num_cols), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations\n",
    "plt.figure(figsize=(11, 8))\n",
    "corr = df[num_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlations fortes\n",
    "print(\"Correlations fortes (>0.7) :\")\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(i+1, len(corr.columns)):\n",
    "        if abs(corr.iloc[i, j]) > 0.7:\n",
    "            print(f\"{corr.columns[i]} <-> {corr.columns[j]}: {corr.iloc[i, j]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation features / cible\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Age\n",
    "axes[0, 0].hist([df[df['y']=='yes']['age'], df[df['y']=='no']['age']], \n",
    "                bins=30, label=['yes', 'no'], color=['#4ecdc4', '#ff6b6b'], alpha=0.7)\n",
    "axes[0, 0].set_title('Age')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Campaign\n",
    "axes[0, 1].hist([df[df['y']=='yes']['campaign'], df[df['y']=='no']['campaign']], \n",
    "                bins=20, label=['yes', 'no'], color=['#4ecdc4', '#ff6b6b'], alpha=0.7, range=(0, 20))\n",
    "axes[0, 1].set_title('Contacts campagne')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Previous\n",
    "axes[0, 2].hist([df[df['y']=='yes']['previous'], df[df['y']=='no']['previous']], \n",
    "                bins=15, label=['yes', 'no'], color=['#4ecdc4', '#ff6b6b'], alpha=0.7, range=(0, 10))\n",
    "axes[0, 2].set_title('Contacts précédents')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "#job\n",
    "job_target = pd.crosstab(df['job'], df['y'], normalize='index') * 100\n",
    "job_target.plot(kind='barh', ax=axes[1, 0], color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1, 0].set_title('Job')\n",
    "axes[1, 0].legend(['no', 'yes'])\n",
    "\n",
    "# Education\n",
    "edu_target = pd.crosstab(df['education'], df['y'], normalize='index') * 100\n",
    "edu_target.plot(kind='barh', ax=axes[1, 1], color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1, 1].set_title('Education')\n",
    "axes[1, 1].legend(['no', 'yes'])\n",
    "\n",
    "# Poutcome\n",
    "pout_target = pd.crosstab(df['poutcome'], df['y'], normalize='index') * 100\n",
    "pout_target.plot(kind='bar', ax=axes[1, 2], color=['#ff6b6b', '#4ecdc4'])\n",
    "axes[1, 2].set_title('Résultat précédent')\n",
    "axes[1, 2].legend(['no', 'yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75a4ad",
   "metadata": {},
   "source": [
    "**Observations :**\n",
    "- Classes très déséquilibrées (~88% no et ~11% yes)\n",
    "- Certaines variables ont des \"unknown\"\n",
    "- Corrélations fortes entre variables économiques, donc possible redondance\n",
    "- La variable \"duration\" ne sert pas a grand chose car elle est connue après l'appel\n",
    "- Le résultat de la campagne précédente est important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad03aa7",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "\n",
    "# on retire duration \n",
    "df_processed = df_processed.drop('duration', axis=1)\n",
    "print(\"Variable duration supprimée\")\n",
    "\n",
    "df_processed['y'] = df_processed['y'].map({'no': 0, 'yes': 1})\n",
    "print(\"Cible encodée (no=0, yes=1)\")\n",
    "\n",
    "# Identifier les types de variables\n",
    "categorical_features = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features.remove('y')\n",
    "\n",
    "print(f\"\\nCatégorielles: {categorical_features}\")\n",
    "print(f\"Numériques: {numerical_features}\")\n",
    "print(f\"\\nShape final: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage variables catégorielles\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col}: {len(le.classes_)} catégories\")\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation du dataset pour les étapes\n",
    "X = df_processed.drop('y', axis=1)\n",
    "y = df_processed['y']\n",
    "\n",
    "print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "print(f\"Classe 0: {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"Classe 1: {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On normalise les features numériques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "print(f\"Normalisation appliquée sur {len(numerical_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf416b69",
   "metadata": {},
   "source": [
    "## 5. Modélisation\n",
    "\n",
    "On va tester 5 algorithmes et comparer leurs performances.\n",
    "1. **Régression Logistique** \n",
    "2. **Decision Tree (Arbre de Décision)** \n",
    "3. **Naive Bayes** \n",
    "4. **Perceptron**\n",
    "5. **K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d175899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation modèles\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', max_depth=10, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Perceptron': Perceptron(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement des modeles\n",
    "import time\n",
    "\n",
    "trained_models = {}\n",
    "predictions = {}\n",
    "prediction_probas = {}\n",
    "training_times = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}...\")\n",
    "    start = time.time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    predictions[name] = y_pred\n",
    "    prediction_probas[name] = y_pred_proba\n",
    "    training_times[name] = train_time\n",
    "    \n",
    "    print(f\"Temps: {train_time:.2f}s\")\n",
    "    print(f\"Accuracy train: {model.score(X_train_scaled, y_train):.3f}\")\n",
    "    print(f\"Accuracy test: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "print(\"\\nEntraînement terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f972ac",
   "metadata": {},
   "source": [
    "## 6. Évaluation et comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "results = []\n",
    "\n",
    "for name in models.keys():\n",
    "    y_pred = predictions[name]\n",
    "    y_pred_proba = prediction_probas[name]\n",
    "    \n",
    "    results.append({\n",
    "        'Modèle': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None,\n",
    "        'Temps (s)': training_times[name]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('F1-Score', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des résultats\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Temps (s)']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    if metric == 'ROC-AUC':\n",
    "        data = results_df[results_df[metric].notna()].sort_values(metric, ascending=False)\n",
    "    else:\n",
    "        data = results_df.sort_values(metric, ascending=False if metric != 'Temps (s)' else True)\n",
    "    \n",
    "    ax.barh(data['Modèle'], data[metric], alpha=0.8)\n",
    "    ax.set_title(metric)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusion TP FP TN FN\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, name in enumerate(models.keys()):\n",
    "    cm = confusion_matrix(y_test, predictions[name])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].set_xlabel('Prédiction')\n",
    "    axes[idx].set_ylabel('Réel')\n",
    "\n",
    "fig.delaxes(axes[5])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6786437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for name in models.keys():\n",
    "    if prediction_probas[name] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, prediction_probas[name])\n",
    "        plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={auc(fpr, tpr):.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Courbes ROC')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05772b99",
   "metadata": {},
   "source": [
    "## 7. Validation Croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917ff80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation avec 5-fold\n",
    "cv_results = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'Modèle': name,\n",
    "        'F1 Moyen': cv_scores.mean(),\n",
    "        'Std': cv_scores.std(),\n",
    "        'Min': cv_scores.min(),\n",
    "        'Max': cv_scores.max(),\n",
    "        'Scores': cv_scores\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}: {cv_scores.mean():.3f} (+-{cv_scores.std():.3f})\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).drop('Scores', axis=1).sort_values('F1 Moyen', ascending=False)\n",
    "print(f\"\\n{cv_df.to_string(index=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6707e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Barres avec écart type\n",
    "data = cv_df.sort_values('F1 Moyen', ascending=True)\n",
    "axes[0].barh(data['Modèle'], data['F1 Moyen'], xerr=data['Std'], capsize=5)\n",
    "axes[0].set_xlabel('F1-Score Moyen')\n",
    "axes[0].set_title('CV Results')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "scores_data = [cv_results[i]['Scores'] for i in range(len(cv_results))]\n",
    "names = [cv_results[i]['Modèle'] for i in range(len(cv_results))]\n",
    "\n",
    "axes[1].boxplot(scores_data, labels=names, patch_artist=True)\n",
    "axes[1].set_ylabel('F1-Score')\n",
    "axes[1].set_title('Distribution par Fold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40330bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features avec Decision Tree\n",
    "dt_model = trained_models['Decision Tree']\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance)), importance['Importance'].values)\n",
    "plt.yticks(range(len(importance)), importance['Feature'].values)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Features - Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Features les plus importantes :\", importance['Feature'].head(5).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9d6f3",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Résultats\n",
    "\n",
    "D'après ces analyses, le **Decision Tree** donne les meilleurs résultats avec un bon équilibre Precision/Recall et une bonne interprétabilité.\n",
    "\n",
    "**Comparaison :**\n",
    "- **Decision Tree** : Meilleures performence, facile à interpréter\n",
    "- **Régression Logistique** : Bon modèle et assez rapide\n",
    "- **KNN** : Correct mais lent\n",
    "- **Naive Bayes** : Rapide mais hypothèse d'indépendance limitante\n",
    "- **Perceptron** : Performances plus faibles\n",
    "\n",
    "### Points importants\n",
    "\n",
    "**Variables influentes :**\n",
    "- Indicateurs économiques **(euribor3m, nr.employed)**\n",
    "- Historique contacts **(pdays, poutcome)**\n",
    "\n",
    "**Problèmes rencontrés :**\n",
    "- Classes déséquilibrées (88% no / 11% yes)\n",
    "- Valeurs \"unknown\" dans certaines variables\n",
    "- Variable \"duration\" exclue \n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "Optimisation des hyperparamètres :\n",
    "   - Decision Tree : tester différentes profondeurs\n",
    "   - KNN : tester différentes valeurs de k \n",
    "   - Perceptron : ajuster le learning rate et le nombre d'itérations\n",
    "\n",
    "Analyse temporelle : \n",
    "   - Modèles prenant en compte l'évolution dans le temps  \n",
    "\n",
    "Feature engineering avancé  \n",
    "\n",
    "Tester d'autres modèles plus performant \n",
    "\n",
    "## Conclusion Finale\n",
    "\n",
    "Ce projet a démontré l'efficacité de plusieurs algorithmes de machine learning pour prédire le succès des campagnes de télémarketing bancaire. Le **Decision Tree** se distingue comme le meilleur choix, offrant un excellent compromis entre performances et stabilité.\n",
    "\n",
    "La **Régression Logistique** constitue également une excellente alternative, avec l'avantage d'être très rapide et robuste, bien que légèrement moins performante.\n",
    "\n",
    "Les modèles développés peuvent significativement améliorer l'efficacité des campagnes marketing en permettant un ciblage plus précis des clients potentiellement intéressés par les dépôts à terme bancaires.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
